# Fraud Detection — End-to-End Case Study

Binary classification of fraudulent e-commerce transactions. Demonstrates the full ML lifecycle from data exploration to production monitoring.

## Objective

Detect fraudulent transactions (chargeback, account takeover) in real-time e-commerce payment flows. The model must produce calibrated probabilities (not just rankings) because downstream business rules apply dollar-amount-dependent thresholds.

## Data

| Property | Value |
|----------|-------|
| **Public dataset** | [IEEE-CIS Fraud Detection](https://www.kaggle.com/c/ieee-fraud-detection) — 590k transactions, 434 features |
| **Production data** | Proprietary (Mercado Livre) — not included. 200M+ rows/month processed. |
| **Synthetic demo** | 10,000 transactions generated by `demo/generate_synthetic.py` |

The public IEEE-CIS dataset is used in the [notebooks](notebooks/). The demo uses synthetic data to run without any external downloads.

## Architecture

```
┌────────────────────────────────────────────────────────────────┐
│                        DATA LAYER                              │
│  BigQuery / Databricks  →  Feature Engineering (parallel)      │
│  200M+ rows/month          IV, KS, mutual info, freq encoding  │
└──────────────────────────────┬─────────────────────────────────┘
                               │
                               ▼
┌────────────────────────────────────────────────────────────────┐
│                      TRAINING LAYER                            │
│  MLflow Tracking  →  Model Training  →  MLflow Registry        │
│  hyperparams, metrics    LightGBM / PyTorch     Staging → Prod │
│                          (DDP, AMP, embeddings)                │
└──────────────────────────────┬─────────────────────────────────┘
                               │
                               ▼
┌────────────────────────────────────────────────────────────────┐
│                     SERVING LAYER                              │
│  Docker Container  →  GCP Vertex AI Endpoint                   │
│  TorchScript/ONNX       Canary rollout: 1% traffic, 24h       │
│  or joblib               Automated rollback on error_rate      │
└──────────────────────────────┬─────────────────────────────────┘
                               │
                               ▼
┌────────────────────────────────────────────────────────────────┐
│                    MONITORING LAYER                             │
│  Prometheus/Grafana  →  Drift Detection  →  Alert Escalation   │
│  latency, error_rate     PSI, KS test        Slack, PagerDuty  │
│  fraud_catch_rate        weekly batch eval    auto-retrain      │
└────────────────────────────────────────────────────────────────┘
```

## Evaluation Metrics

| Metric | Target | Why |
|--------|--------|-----|
| ROC-AUC | > 0.95 | Discrimination — can the model separate fraud from legitimate? |
| Precision @ Recall=0.70 | > 0.50 | Business constraint — 70% fraud catch rate with tolerable false alarms |
| Brier Score | < 0.03 | Calibration — are predicted probabilities accurate? |
| ECE | < 0.05 | Calibration gap across probability bins |

## Deployment Strategy

1. **Export**: LightGBM via joblib; PyTorch via TorchScript or ONNX
2. **Containerise**: Docker image with FastAPI serving endpoint
3. **Deploy**: GCP Vertex AI endpoint with traffic splitting
4. **Canary**: 1% traffic for 24 hours, monitor `error_rate`, `latency_p95`, `fraud_catch_rate`
5. **Rollback**: Automated if `error_rate > 0.5%` above baseline or `latency_p95 > 200ms`
6. **Ramp**: 100% traffic after clean 24h window

## Monitoring & Rollback

- **Drift**: PSI + KS test on top features, computed daily. PSI ≥ 0.20 → auto-retrain ticket.
- **Performance**: Weekly batch eval on labelled cohort (30-day chargeback dispute lag). AUC drop ≥ 3% for two weeks → retrain.
- **Rollback**: Vertex AI traffic split to previous model version (< 60s).

## Run the Demo

```bash
bash fraud_detection/demo/run_demo.sh
```

Produces `fraud_detection/demo/results/`:
- `synthetic_data.csv` — 10,000 synthetic transactions
- `summary.json` — evaluation metrics
- `calibration_curve.png` — reliability diagram

## Related Files

- [Notebooks](../notebooks/) — PyTorch NN, SHAP, Evidently AI analysis
- [Model Card](model_card.md) — model documentation
- [Architecture Decisions](architecture.md) — trade-off analysis
- [ds_tools](../../ds_tools/) — reusable evaluation and monitoring toolkit
